"""
Token estimation for conversation processing.

This module provides tiktoken-based token counting for fallback when
providers don't return usage data.
"""

import typing as _typing

import tiktoken as _tiktoken


def get_encoder(model: str) -> _tiktoken.Encoding:
    """
    Get a tiktoken encoder for the given model.

    Uses tiktoken's model-to-encoding mapping where available, with fallback
    to cl100k_base for unknown models.

    Args:
        model: Model name (e.g., "gpt-oss-120b", "gpt-4", "claude-3-sonnet")

    Returns:
        A tiktoken Encoding suitable for the model.
    """
    try:
        return _tiktoken.encoding_for_model(model)
    except KeyError:
        # Unknown model - use reasonable default
        # cl100k_base is GPT-4's tokenizer, reasonable approximation for most LLMs
        return _tiktoken.get_encoding("cl100k_base")


def count_tokens(encoder: _tiktoken.Encoding, text: str) -> int:
    """
    Count the number of tokens in a text string.

    Args:
        encoder: A tiktoken Encoding instance.
        text: Text to count tokens for.

    Returns:
        Number of tokens.
    """
    return len(encoder.encode(text))


def count_messages_tokens(
    encoder: _tiktoken.Encoding,
    messages: list[dict[str, _typing.Any]],
    system_prompt: str | None = None,
) -> int:
    """
    Estimate token count for a list of messages.

    This provides a rough estimate of input tokens (context size) before an API call.
    The actual count from the provider may differ due to:
    - Provider-specific formatting
    - Special tokens for roles
    - Tool definitions overhead

    Args:
        encoder: A tiktoken Encoding instance.
        messages: List of message dicts with 'role' and 'content'.
        system_prompt: Optional system prompt to include.

    Returns:
        Estimated token count.
    """
    import json as _json

    total = 0

    # Count system prompt
    if system_prompt:
        total += count_tokens(encoder, system_prompt)
        total += 4  # Approximate overhead for system role wrapper

    # Count each message
    for msg in messages:
        # Role overhead (approximate)
        total += 4

        # Content - can be string or list of content blocks
        content = msg.get("content", "")
        if isinstance(content, str):
            total += count_tokens(encoder, content)
        elif isinstance(content, list):
            # Content blocks (text, tool_use, tool_result, etc.)
            for block in content:
                if isinstance(block, dict):
                    if "text" in block:
                        total += count_tokens(encoder, block["text"])
                    elif "content" in block:
                        # Tool result content
                        result_content = block["content"]
                        if isinstance(result_content, str):
                            total += count_tokens(encoder, result_content)
                    elif "input" in block:
                        # Tool use - count the JSON input
                        try:
                            total += count_tokens(encoder, _json.dumps(block["input"]))
                        except (TypeError, ValueError):
                            total += 50  # Fallback estimate

    return total


class ConversationTokenTracker:
    """
    Tracks token estimates throughout a conversation for fallback logging.

    This tracker maintains tiktoken-based estimates that can be used when
    the provider doesn't return usage data.
    """

    def __init__(self, model: str) -> None:
        """
        Initialize with a model name for encoder selection.

        Args:
            model: Model name for tokenizer selection.
        """
        self._encoder = get_encoder(model)
        self._model = model
        self._current_turn_output = 0

    def estimate_context_tokens(
        self,
        messages: list[dict[str, _typing.Any]],
        system_prompt: str | None = None,
    ) -> int:
        """
        Estimate input/context tokens before an API call.

        Args:
            messages: Messages to send to the API.
            system_prompt: System prompt if any.

        Returns:
            Estimated input token count.
        """
        return count_messages_tokens(self._encoder, messages, system_prompt)

    def reset_turn(self) -> None:
        """Reset output token counter for a new turn."""
        self._current_turn_output = 0

    def add_output_text(self, text: str) -> int:
        """
        Add output text and return updated output token count.

        Args:
            text: Text generated by the model.

        Returns:
            Updated output token count for this turn.
        """
        tokens = count_tokens(self._encoder, text)
        self._current_turn_output += tokens
        return self._current_turn_output

    @property
    def current_turn_output(self) -> int:
        """Current estimated output tokens for this turn."""
        return self._current_turn_output

    @property
    def encoder_name(self) -> str:
        """Name of the encoder being used."""
        return self._encoder.name

